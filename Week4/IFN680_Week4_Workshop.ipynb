{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# ML that can See: Supervised Learning with Images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "Let's load in any libraries we will use in this notebook. We're also going to install a new package called weights and biases (wandb) -- more on that later in the practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tqdm\n",
    "\n",
    "!pip install wandb\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743d496-b334-499f-b57f-da86a441f925",
   "metadata": {},
   "source": [
    "# Part 1: Prepare the Data\n",
    "## Inspect the Data\n",
    "**Make sure you've extracted the dataset folder by right-clicking and selecting 'Extract Archive'**. Once you've done this, look at the different folders and how the dataset is structured. Click open some images to see what they look like and get a feel for the data you're going to be working with.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "This step has 2 key parts:\n",
    "1. Create default transformations to apply to the data. The below 3 steps are very standard, and should always be used.\n",
    "    There are a number of transformations we will consider here, these include:\n",
    "    1. [transforms.ToTensor()](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html) -- this converts a PIL image or numpy array to a tensor while scaling the pixel values to the range [0, 1].\n",
    "    2. [transforms.Resize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) -- this resizes an input image to the specified size (height, width).\n",
    "    Resize is important as it ensures the dimensions remain compatible throughout the network, allowing proper operations at each layer and maintaining the required dimensions for the final fully connected layers in the network.\n",
    "    3. [transforms.Normalize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html) -- this standardizes the pixel values of a tensor image by subtracting the mean and dividing by the standard deviation along the input channels.\n",
    "    \n",
    "    You can then use transforms.Compose to sequentially chain multiple transforms together.\n",
    "    \n",
    "2. Load the datasets in with [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) -- this loads image datasets from folders, assigning labels automatically based on subdirectories, making it convenient for tasks like image classification. If you examine our dataset in the ASL_DatasetSubset/Train folder, you'll notice that the data is stored in a subdirectory for each class - perfect for this function!\n",
    "\n",
    "**Why Resize to 224x224?**\n",
    "Many popular pre-trained models, such as AlexNet, VGG, and ResNet, were trained on the ImageNet dataset, which used images of size 224x224 pixels. We will use a ResNet architecture pre-trained on ImageNet on this practical, so will use this value.\n",
    "\n",
    "**Why Normalize with mean and standard deviation of 0.5?**\n",
    "It is common practice of using the value 0.5 for both mean and standard deviation. It's important to note that while the choice of 0.5 for mean and standard deviation is a common default, it might not be the best choice for all cases. The optimal values could vary depending on the specific dataset and task. In some scenarios, it's advisable to calculate the actual mean and standard deviation from your dataset and use those values for normalization. This could lead to better results, especially if the dataset has significantly different characteristics than the data the normalization parameters were originally chosen for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14c9ab-3029-4195-932f-af7ee5666c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias = True), \n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "entire_dataset = torchvision.datasets.ImageFolder('stanford_dogs_subset/', transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade2443-cf35-467e-83f8-4b07cde105b6",
   "metadata": {},
   "source": [
    "You also might want to extract some key details about the dataset -- for example, what are the class labels and how many classes are there?\n",
    "\n",
    "You can do that with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378984c-7f68-42c8-9595-35a3d55f5070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_labels = entire_dataset.classes\n",
    "num_classes = len(class_labels)\n",
    "\n",
    "print(f'Dataset has {num_classes} classes, which are: {class_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b12388-1c03-4438-a16e-4b8dcb15b85e",
   "metadata": {},
   "source": [
    "## Split the Data\n",
    "\n",
    "Below, I've split the total dataset into two subsets: a train+val subset and a test subset, using the torch.utils.data.random_split() function.\n",
    "\n",
    "**Your turn:** Use [torch.utils.data.random_split()](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to randomly separate the ```trainval_dataset``` into a training and validation subset. \n",
    "\n",
    "Try using the documentation first, and if you're stuck there is extra support in the IFN680 Practical Support Sheet.\n",
    "\n",
    "If you solved this in the Week 3 practical sheet, you should be able to adapt that code again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843dbdf-8cc5-43e8-83e4-3457eafd2d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_portion = 0.3\n",
    "test_size = int(test_portion*len(entire_dataset))\n",
    "trainval_size = len(entire_dataset)-test_size\n",
    "trainval_dataset, test_dataset = torch.utils.data.random_split(entire_dataset, [trainval_size, test_size])\n",
    "\n",
    "train_portion = 0.7\n",
    "##### Your code goes here ######\n",
    "\n",
    "\n",
    "print(f'Size of train dataset: {len(train_dataset)}')\n",
    "print(f'Size of val dataset: {len(val_dataset)}')\n",
    "print(f'Size of test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d241d-edcc-467d-a608-9af725783e03",
   "metadata": {},
   "source": [
    "## Visualise the data and class distribution\n",
    "\n",
    "It's important to get a feel for the data by visualising it, and also to understand any underlying characteristics -- e.g. is there any bias you can detect in the data that might limit how it can be used in the future? what is the balance of different classes in the dataset? It's also good to check that things are still looking similar to how they were in the image files in the folders, to check that nothing in the resize or normalisation process has gone wrong.\n",
    "\n",
    "In the cell below, I'm using matplotlib.pyplot to visualise some images with the subplot() function -- there is more information on this in the IFN680 Practical Support Sheet. I'm just going to visualise the first 5 images in the dataset -- you could do more, or randomly sample images from the dataset, to get a more representative view.\n",
    "\n",
    "**Your turn**: Use a histogram function to visualise the distribution of class labels in the training and validation dataset, and check how consistent it is between these two subsets. If this is very imbalanced, you may want to randomly generate the train/val split again in the cell above. You can use code from the Week 3 practical sheet, and also look at the IFN680 Practical Support Sheet for more details on the plt.hist() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17099f26-b198-43cc-844f-885634ee2dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5)\n",
    "for idx in range(5):\n",
    "    train_image = (train_dataset[idx][0].numpy())/2 + 0.5\n",
    "    label = class_labels[train_dataset[idx][1]]\n",
    "    train_image = np.moveaxis(train_image, 0, 2)\n",
    "    ax[idx].imshow(train_image)\n",
    "    ax[idx].set_axis_off()\n",
    "    ax[idx].set_title(label.split('-')[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#### Your code goes below to create the histogram of class distributions between the training and validation data subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b7ba5-55b5-4350-abe2-9eb88cc38537",
   "metadata": {},
   "source": [
    "# Part 2: Initialise the model, dataloaders, loss function, and optimiser\n",
    "\n",
    "Now that we've inspected the data and done our initial pre-processing, we need to initialise some other important things before we can begin training. These include:\n",
    "1. Initialise the model we will use for classification.\n",
    "2. Adapt the model for transfer learning\n",
    "2. Initialise the loss function we will use to supervise the training of our model.\n",
    "3. Initialise the optimiser (stochastic gradient descent) that will update parameters for us.\n",
    "4. Initialise the dataloaders which will batch our datasets for testing.\n",
    "\n",
    "## Initialise the model\n",
    "We will use a pretrained ResNet18, that has been trained on ImageNet. This is very easy to do in PyTorch -- ```torchvision.models.resnet18``` loads the architecture, and using ```weights=ResNet18_Weights.IMAGENET1K_V1``` loads the trained parameters for the model after it was trained on ImageNet.\n",
    "\n",
    "Below, I'm printing out the model so that you can see the architecture -- look through the architecture, and try to identify elements we talked about in the lecture -- Convolutional layers, ReLU, pooling layers, batch normalisation, and the final linear (or fully connected layer). You can read more about these layers in torch in the IFN680 Practical Support Sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2943b5-399d-4e6b-970b-909859cd440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d18195-7945-4675-b9b3-553f39e69dec",
   "metadata": {},
   "source": [
    "## Adapt the model architecture for transfer learning\n",
    "\n",
    "In the printed model definition above, the final Linear layer is taking an input of size 512 (in_features) and using 1000 neurons (out_features) to create 1000 class scores. The model was created this way because it was trained to perform image classification on ImageNet, which has 1000 classes.\n",
    "\n",
    "We have a variable ```num_classes``` that is storing how many classes our new dataset has, and how many class scores we want to generate. \n",
    "\n",
    "**Your turn**: Complete the code block below, changing the last layer of the model to only produce ```num_classes``` class scores.\n",
    "\n",
    "Hint: You can re-assign the final layer by creating a new linear layer using [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). The number of input features will not change, but the number of neurons or out_features should. When you ```print(model.fc)```, you should see a Linear layer with 512 in_features and 13 out_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1bce78-34ae-4849-9c79-07ee79925125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Your code goes here to change the final layer of our model\n",
    "model.fc = ...\n",
    "\n",
    "print(model.fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb130282-c5f0-4f6b-9160-4f518f2439d3",
   "metadata": {},
   "source": [
    "Now that our model is adapted for transfer learning, we should also load the model onto our GPU if it is available -- this will massively speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e012c1-f651-41be-823c-7e9fae1365e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae628b-d332-4717-afe9-b070e2b9a871",
   "metadata": {},
   "source": [
    "## Initialise the loss function, data loaders and optimiser\n",
    "\n",
    "In the cell below, I've created a ```criterion``` variable that holds the instantiated [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). You can read more about it at the documentation or in the IFN680 Support Practical Sheet.\n",
    "\n",
    "I've also created the Data Loaders -- these process the dataset into mini-batches, ready for stochastic gradient descent. You can see the IFN680 Support Practical Sheet for more details about Data Loaders, and the IFN680 Week 3 lecture slides for more details about mini-batches. I've chosen a batch size of 8 -- a bigger batch size is usually better, but to big means we don't have enough GPU space. 8 seemed to work well with our cluster GPUs. A number between 4-128 is usually reasonable depending on the size of your GPU and the size of your images.\n",
    "\n",
    "Finally, I've instantiated a stochastic gradient descent optimizer using [optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). You can also read more about it in the IFN680 Practical Support Sheet. I'm starting with a learning rate of 0.001 and a momentum of 0.9 -- these are reasonable starting values, but we may have to play around with these down the line to get best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55884c66-64f9-42f8-8ee4-d30e08690929",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers = 1)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers = 1)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers = 1)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61498737-b776-4a8d-80d6-e8d2f72f90f9",
   "metadata": {},
   "source": [
    "# Part 3: Transfer Learning with our new dataset\n",
    "\n",
    "## Optional: Initialise a Weights and Biases visualisation \n",
    "\n",
    "Last week in the practical, you used matplotlib.pyplot and the plot() function to visualise the loss and accuracy of the model as you trained. This is suitable for small-scale projects, but when you're spending longer times training, you might want to visualise the loss and accuracy as it trains! For example, what if your training de-stabilised, and you didn't realise until you'd spent 5 hours training? You also might want to experiment with many different hyperparameters, such as learning rate, and want a useful way to organise the different results. There are many ways to do this, the weights and biases library is one of those. This is **optional** -- you can continue to use the plot() function of pyplot if you'd prefer. Please also be prepared to engage with the many tutorials and guides that are well-documented on the wandb website -- for example, this 'quick start' guide: https://docs.wandb.ai/quickstart\n",
    "\n",
    "To use ```wandb```, you must **first setup an account**: Follow this link to 'sign up' and create an account - https://wandb.ai/site\n",
    "\n",
    "To create the project, I'm doing 2 things:\n",
    "1. Checking I am logged in -- if this is the first time, you will need to enter your login credentials\n",
    "2. Initialise a training run with relevant details. e.g. you can pass in an overall project name (‘Week 4 Practical’, or ‘Project 1’), a name for the training run you are about to conduct ('transfer learning fine-tune'), and also a config detailing the hyperparameters that you are going to use during training (learning rate, batch size, other optimizer hyperparameters). \n",
    "\n",
    "You can login to your wandb account on their website to view plots created from different training sessions of a model, or click the links below after running the code.\n",
    "\n",
    "**Your turn:** update the values in the config dictionary to match what you have initialised above!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ffd68-bf65-4de2-a8f9-2e8c80c6f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "\n",
    "#Complete the below to match what you initialied in the previous cell\n",
    "config = {\n",
    "    \"learning_rate\": 0,\n",
    "    \"momentum\": 0,\n",
    "    \"batch_size\": 0\n",
    "    }\n",
    "\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"Week 4 practical worksheet\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=\"transfer_learning_finetune\", \n",
    "    # Track hyperparameters and run metadata\n",
    "    config=config )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d207335-fe2e-4b0b-ac51-6e41528b136b",
   "metadata": {},
   "source": [
    "## Transfer Learning with fine-tuning\n",
    "Now that we've initialised our model, adapted it's architecture for our new training dataset, and initialised the other elements of training (stochastic gradient descent optimizer and cross-entropy loss), we can start to train our model.\n",
    "\n",
    "We're first going to use a fine-tuning approach, where the model's parameters are adjusted slightly to adapt its learned features to the specific nuances of the new task or domain. We're going to adjust the parameters in every layer of the network (i.e. do not freeze any layers).\n",
    "\n",
    "The code in the cell below is very similar to what we had in Week 3 -- this is because it is the general training process you can always use for deep learning with PyTorch. It has the following steps:\n",
    "\n",
    "**For each epoch (i.e. iteration over the entire dataset)\n",
    "1. Set the model to 'train' mode. Modes ('train' or 'eval') are used to control the behavior of models, especially when dealing with models that have different behaviors during training and evaluation (testing) phases, usually due to dropout and batch normalization layers, which behave differently during training and evaluation. \n",
    "2. Grab the next batch in the trainloader (multiple images and their ground-truth labels)\n",
    "    1. Move the data to the GPU if it is available.\n",
    "    2. Zero the parameter gradients stored in the optimizer (stops gradients of previous batches affecting optimization).\n",
    "    3. Pass the input data through the model to produce predictions.\n",
    "    4. Calculate the loss by passing the predictions and ground-truth labels through the instantiated loss function.\n",
    "    5. Complete a backward pass that calculates the gradients of each parameters with respect to the current loss.\n",
    "    6. Use gradient descent (via the optimizer) to adjust the parameters based on the gradients from the prior step.\n",
    "    7. Store any data that describes training process as necessary (e.g. loss or accuracy or other performance metrics).\n",
    "3. For any data you're using for training curves, calculate the epoch-representative value and then store or log this data. Here I am logging with ```wandb.log()```. This will use the previously initialised run, where I pass in a python dictionary of the values to plot, as well as the current step in training (i.e. epoch).\n",
    "    \n",
    "\n",
    "**If the concepts described above do not feel familiar to you theoretically, please revise the Week 3 and Week 4 lecture.**\n",
    "\n",
    "This code will take about 2 minutes to run -- make sure you check the accuracy and loss curves on the [wandb website](https://wandb.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e310fc6-cb70-4354-8745-7b5393ef1d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_epochs = 5\n",
    "\n",
    "for epoch in range(total_epochs):    \n",
    "    #1\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #2 \n",
    "    for i, data in  tqdm.tqdm(enumerate(trainloader, 0), total = len(trainloader), desc = f'Epoch {epoch+1} - training phase'):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #A. move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #B. zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #C.  forward pass to find the outputs\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #D. calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #E. backward pass to calculate the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #F. take a step with gradient descent to change the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #G. let's keep track of the loss and the accuracy\n",
    "        train_loss += [loss.cpu().item()]\n",
    "        \n",
    "        # To find the accuracy, we need to know which label is predicted by our model. This is the class with the highest class score.\n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        \n",
    "        #now we'll count how many were correct vs how many there were - we can use this later to find the total accuracy of the epoch\n",
    "        correct += torch.sum(predicted == labels).cpu().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    #3. record the mean loss and accuracy over the entire epoch and training dataset\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    train_accuracy = correct/total\n",
    "    \n",
    "    #log with wandb\n",
    "    wandb.log({\"training_loss\": mean_train_loss, \"training_accuracy\": train_accuracy}, step = epoch)\n",
    "    \n",
    "    \n",
    "    #### YOUR CODE FOR THE VALIDATION DATASET GOES BELOW\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d37b39-1a9b-465c-9e55-cc1896c7af94",
   "metadata": {},
   "source": [
    "## Your turn: Add in the validation dataset check!\n",
    "\n",
    "The above loop trains our model, and if you visualise the results of the loss curves, it should look like it is doing really well on the training dataset. After 1 epoch, I have a training accuracy of approximately 96.5%. So is our dataset really easy, or are we overfitting on the training data?\n",
    "\n",
    "To answer this, we need to check the performance on the validation dataset.\n",
    "\n",
    "Go back to the cell above, and add in the validation loop. It should have the following steps:\n",
    "\n",
    "**For each epoch (i.e. iteration over the entire dataset) -- this code already exists above, nestle the code below within it!\n",
    "1. Set the model to 'eval' mode with ```model.eval()```. Modes ('train' or 'eval') are used to control the behavior of models, especially when dealing with models that have different behaviors during training and evaluation (testing) phases, usually due to dropout and batch normalization layers, which behave differently during training and evaluation. \n",
    "2. Grab the next batch in the valloader (multiple images and their ground-truth labels)\n",
    "    1. Move the data to the GPU if it is available.\n",
    "    3. Pass the input data through the model to produce predictions.\n",
    "    4. Calculate the loss by passing the predictions and ground-truth labels through the instantiated loss function.\n",
    "    7. Store any data that describes training process as necessary (e.g. loss or accuracy or other performance metrics).\n",
    "3. For any data you're using for validation curves, calculate the epoch-representative value and then store or log this data.\n",
    "\n",
    "For the above steps, you can follow a similar approach to the training loop **BUT** make sure you correctly name your variables, and at no point should you use the optimizer with your validation data (i.e. update parameters based on performance on the validation data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39367729-1230-45e4-be14-d7c9f9742c8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check-in: Using the same training hyperparameters, what validation accuracy did your model achieve?\n",
    "\n",
    "Was your model overfitting to the training dataset? How did the validation accuracy and loss curve compare? \n",
    "\n",
    "I found that my validation accuracy was fluctuating around 83-85% -- there's definitely some overfitting going on as the training accuracy was much higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb916c-6e6a-4d59-8873-3c5da1e2c165",
   "metadata": {},
   "source": [
    "## Re-loading our training dataset with data augmentations\n",
    "\n",
    "There are a couple of things that can help with overfitting -- one would be making sure that we've added weight regularisation terms to our optimizer.\n",
    "\n",
    "In this cell, we're going to try something new and add some data transformations. I've already added one interesting data transformation to the list of transforms for the training dataset. Go through the additional data transformation functions below, and choose some additional transforms to apply. Remember to tailor the transformations to the characteristics the dataset. We also only apply data augmentation to the training set, while the validation and test sets will use the standard transform that does not include data augmentation.\n",
    "\n",
    "**Your turn: Pick additional data transforms and add them to the train_transforms list!**\n",
    "\n",
    "1. (already implemented below) [transforms.RandomResizedCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomResizedCrop.html) -- this function randomly grabs a portion of the image (crops) and then resizes to the desired image size. By default, the crop can be anywhere between 8% to 100% of the image original area -- this is a little strict, I'm going to choose between 50% and 100% of the image area.\n",
    "2. [transforms.RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html) -- randomly flips an image horizontally. Useful for tasks where horizontal orientation doesn't change the meaning.\n",
    "3. [transforms.RandomVerticalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomVerticalFlip.html) -- randomly flips an image vertically. Useful for tasks where vertical orientation doesn't change the meaning.\n",
    "4. [transforms.RandomRotation](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomRotation.html) -- randomly rotates an image by a specified angle. Can simulate variations in viewpoint.\n",
    "5. [transforms.ColorJitter](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) -- randomly changes brightness, contrast, saturation, and hue of an image. Helps the model to be robust to different lighting conditions.\n",
    "\n",
    "Try to complete this task using the PyTorch documentation. Otherwise if you're stuck, there is additional support information in the IFN680 Practical Support Sheet.\n",
    "\n",
    "Once we've done this, there's a few way to use this transform -- we can add it into the training loop (what we will do), or you can make a custom dataset that automatically applies this transformation for the training subset (see here: https://discuss.pytorch.org/t/transforms-on-subset/166836)\n",
    "\n",
    "**What's going on here?**\n",
    "\n",
    "Each of the 'Random' transformations will be sequentially applied to an input image, with different transformations of different severities - the severity of the transformation is the random component. When you chain together multiple different types of 'Random' transformations, we can end up with a huge variation of different images from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa96a7-0bcf-4d9c-a79a-913d1d02b292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomResizedCrop((224, 224), scale = (0.5, 1)),\n",
    "    ])\n",
    "\n",
    "\n",
    "#visualise the train dataset with these transforms\n",
    "data = next(iter(trainloader))\n",
    "fig, ax = plt.subplots(1, 5)\n",
    "for idx in range(5):\n",
    "    im = data[0][idx]\n",
    "    lbl = data[1][idx]\n",
    "    im = train_transform(im)\n",
    "    train_image = (im.numpy())/2 + 0.5\n",
    "    label = class_labels[lbl]\n",
    "    train_image = np.moveaxis(train_image, 0, 2)\n",
    "    ax[idx].imshow(train_image)\n",
    "    ax[idx].set_axis_off()\n",
    "    ax[idx].set_title(label.split('-')[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28236d2-4e8b-4237-a59b-43b147c0f752",
   "metadata": {},
   "source": [
    "## Train with Data Augmentation\n",
    "\n",
    "First, let's re-initialise a ```wand``` run to explain what this new network's hyperparameters are, and the difference to the previous run. Make sure you add in the details of any extra transformations you are using to the config variable.\n",
    "\n",
    "Then, we also need to re-initialise our model and optimizer -- we currently have an already trained ResNet18, we want to check how performance changes with this new set of data augmentations! We also need to make sure the optimizer is conditioned on these new parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9c287-9072-4730-88b6-05f45c03c547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Complete the below to match what you initialied in the previous cell\n",
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 8,\n",
    "    \"augmentation\": ['random resized crop, scale 0.5-1']\n",
    "    }\n",
    "\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"Week 4 practical worksheet\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=\"transfer_learning_finetune_data_augmentation\", \n",
    "    # Track hyperparameters and run metadata\n",
    "    config=config)\n",
    "\n",
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2848579-5dd7-444c-a8d2-e81de534cf21",
   "metadata": {},
   "source": [
    "Below is the training code from above, except now I am also applying the transform to the inputs before they are fed into the model! Because we are adding transformations, you may find you also need to train the model for slightly longer -- I've increased the number of ```total_epochs``` to 10.\n",
    "\n",
    "Copy and paste your code for testing and logging performance on the validation dataset into this loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ee5df-f824-43e4-a41d-d93792401ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_epochs = 10\n",
    "\n",
    "for epoch in range(total_epochs):    \n",
    "    #1\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #2 \n",
    "    for i, data in  tqdm.tqdm(enumerate(trainloader, 0), total = len(trainloader), desc = f'Epoch {epoch+1} - training phase'):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #A. move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #NEW: Apply our data augmentation here\n",
    "        inputs = train_transform(inputs)\n",
    "        \n",
    "        #B. zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #C.  forward pass to find the outputs\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #D. calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #E. backward pass to calculate the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #F. take a step with gradient descent to change the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #G. let's keep track of the loss and the accuracy\n",
    "        train_loss += [loss.cpu().item()]\n",
    "        \n",
    "        # To find the accuracy, we need to know which label is predicted by our model. This is the class with the highest class score.\n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        \n",
    "        #now we'll count how many were correct vs how many there were - we can use this later to find the total accuracy of the epoch\n",
    "        correct += torch.sum(predicted == labels).cpu().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    #3. record the mean loss and accuracy over the entire epoch and training dataset\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    train_accuracy = correct/total\n",
    "    \n",
    "    #log with wandb\n",
    "    wandb.log({\"training_loss\": mean_train_loss, \"training_accuracy\": train_accuracy}, step = epoch)\n",
    "    \n",
    "    \n",
    "    ### COPY AND PASTE VALIDATION LOOP CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d78a92-614c-401c-a907-90a5274a54a6",
   "metadata": {},
   "source": [
    "## Food for thought\n",
    "1. Did your data augmentations improve generalisation? Mine didn't -- I got a lower validation accuracy than before (only 82%). This can happen! It means I didn't choose good data augmentations for generalisation -- I only used the RandomResizedCrop augmentation. There is always some experimentation with this process, and often your first attempt doesn't work.\n",
    "2. Can you justify why you picked the data augmentation transformations that you used? Why would these transformations provide better generalisation for this dataset? This will be an important thing to think about for Project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827be78-6509-4b09-ae25-fc13c0574ab2",
   "metadata": {},
   "source": [
    "## Transfer Learning with freezing layers\n",
    "\n",
    "Freezing layers in a PyTorch model involves setting the ```requires_grad``` (i.e. requires gradient) attribute of the parameters in those layers to ```False```. This prevents the parameters from being updated during training by the optimizer. You can selectively freeze layers and train only the desired layers, such as the last fully connected layer. \n",
    "\n",
    "Below, I'm first re-initialising a fresh instance of ResNet18. I'm then going through every layer in the model and setting the ```requires_grad``` attribute to False. Then, I change only the last fully-connected layer ```requires_grad``` attribute to True.\n",
    "\n",
    "If you want to look at model parameter names, you can always ```print(model)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4b9c8-97e9-4684-811e-301a44c77000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the parameters of the last fully connected layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475ae37-8e48-4f00-b60d-4b295c7982fc",
   "metadata": {},
   "source": [
    "**Your turn:** Below, follow these steps to get results for transfer learning with freezed layers:\n",
    "1. Re-initialise optimizer\n",
    "2. Re-initialise a run with a suitable name in wandb\n",
    "3. Copy and paste the training and validation loops from earlier to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6dc0d-954a-4465-af2f-d30dca455dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Your code goes below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9617f0b-d9d9-47a3-9eda-53ed5afb27a6",
   "metadata": {},
   "source": [
    "## Check in -- how did transfer learning with freezing compare to transfer learning with fine-tuning?\n",
    "\n",
    "I found that transfer learning with fine-tuning and freezing both achieved validation accuracy around 85%. Not a big difference. This is something that will change depending on the dataset, it is always a good idea to experiment with both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0df818-6b6c-4ec9-b238-65b302b903e2",
   "metadata": {},
   "source": [
    "## Saving the best weights\n",
    "\n",
    "Once you've trained a model, you might want to save the model parameters so that you can use it at a later date or even do more training later! You can do this with the [torch.save](https://pytorch.org/docs/stable/generated/torch.save.html) function. \n",
    "\n",
    "I've demonstrated how to save the model parameters, and how you can re-load them at a later date.\n",
    "\n",
    "**Your turn:** Rather than saving the weights after the model is finished training, could you incorporate this into the training process so that you save the weights which achieve the highest validation accuracy? e.g. What if you train for 10 epochs, but your best validation accuracy was on epoch 8?\n",
    "\n",
    "Want extra support? Read the IFN680 Practical Support Sheet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590e994-7ccf-4cc0-a8c8-9b0b7b9cbc56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save the model weights\n",
    "torch.save(model.state_dict(), 'Week4_ResNet_freezed.pth')\n",
    "\n",
    "#load the model weights back into the model\n",
    "model.load_state_dict(torch.load('Week4_ResNet_freezed.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a2ecd-82f3-4f34-a9bf-ceb6a3e0cbab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Food for thought\n",
    "\n",
    "1. Our code above could be cleaned up significantly to allow for many different test iterations -- how could you move some of the code into functions to create a cleaner script (cleaner scripts, fewer bugs!)? For example, could you have a generic init_model() function? What about a train_step() function?\n",
    "2. It's important to get a feel for how the learning rate and number of epochs change performance -- even if it seems like the first number you use works well, always try training for a little longer and always try a few different learning rate values.\n",
    "3. What's the performance on the test dataset? Can you use a confusion matrix to see which classes are being confused for each other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c05f63-90a8-44b3-bbae-4ec824d1f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
