{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3df49f-2858-4c48-ab97-9aa2ccf5988e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q Learning with Frozen Lake\n",
    "\n",
    "In this Notebook, we'll implement a Q Learning agent <b>that plays FrozenLake.</b>\n",
    "\n",
    "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, **so you won't always move in the direction you intend (stochastic environment)**\n",
    "\n",
    "<img src=\"https://www.gymlibrary.dev/_images/frozen_lake.gif\" alt=\"Environments\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5030f00-c908-4984-95f3-114fdbb19080",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import the dependencies\n",
    "\n",
    "We use 3 libraries:\n",
    "\n",
    "- Numpy for our Qtable\n",
    "- OpenAI Gym for our FrozenLake Environment\n",
    "- Random to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d0c0f1-e181-4fa2-a004-13ebb745d803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632addf-7ad5-4464-8f95-4867fb1ab262",
   "metadata": {},
   "source": [
    "## Step 1: Create the environment üéÆ\n",
    "- Here we'll create the FrozenLake 8x8 environment. \n",
    "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
    "- In our case we choose to use Frozen Lake.\n",
    "\n",
    "### Action Space\n",
    "The agent takes a 1-element vector for actions. The action space is (direction), where it decides direction to move in which can be:\n",
    "\n",
    "- 0: LEFT\n",
    "- 1: DOWN\n",
    "- 2: RIGHT\n",
    "- 3: UP\n",
    "\n",
    "\n",
    "### Observation Space\n",
    "The observation is a value representing the agent‚Äôs current position as current_row * nrows + current_col (where both the row and col start at 0). For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. For example, the 4x4 map has 16 possible observations. For example this is what state 0 looks like:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">\n",
    "\n",
    "### Rewards\n",
    "Reward schedule:\n",
    "- Reach goal (G): +1\n",
    "- Reach hole (H): 0\n",
    "- Reach frozen (F): 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca845234-4675-496d-83d6-e5747a6d46d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08dfb5-1f6b-4308-a526-f5a876ed1291",
   "metadata": {},
   "source": [
    "## Step 2: Create the Q-table and initialize it üóÑÔ∏è\n",
    "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n",
    "- Use this information to initialise the correct size grid with zeros for the Q-table. Hint: Use np.zeros()\n",
    "- Remember in the beginning, our Q-Table is useless since it gives arbitrary value for each state-action pair (most of the time we initialize the Q-Table to 0 values). But, as we‚Äôll explore the environment and update our Q-Table it will give us better and better approximations.\n",
    "- Note how the Q-Table is structured in the example below and create a similar table for your FrozenLake environment below.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35efa77-fdfe-4787-a41b-57e7e918b495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3794460943.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    qtable = # TODO: Initialise the Q-Table with zeros\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "# Create our Q table with state_size rows and action_size columns (state_size x action_size)\n",
    "qtable = # TODO: Initialise the Q-Table with zeros\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5b802-1eb3-4535-85e1-47231a6e9c46",
   "metadata": {},
   "source": [
    "## Lets try and play the game where we select actions randomly and see what our success rate is of winning the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fcf079b-783e-468c-8cf8-254fa9ae1bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 7\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 2\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 2\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 13\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 2\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 1\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 2\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 19\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 2\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 11\n",
      "Success Rate:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "successes = 0\n",
    "num_test_episodes = 10\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(99):\n",
    "        \n",
    "        # Sample a random action to take in the environment\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            # env.render()\n",
    "            if new_state == 15:\n",
    "                print(\"We reached our Goal üèÜ\")\n",
    "                successes += 1\n",
    "            else:\n",
    "                print(\"We fell into a hole ‚ò†Ô∏è\")\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            \n",
    "            break\n",
    "        state = new_state\n",
    "        \n",
    "print(\"Success Rate: \", (successes/num_test_episodes)*100 , \"%\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d53ad-607a-495c-a307-7434e5a193c2",
   "metadata": {},
   "source": [
    "## Step 3: Create the hyperparameters ‚öôÔ∏è\n",
    "- Here, we'll specify the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cb201d8-00ac-4e6f-affe-efb7bb66f07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_episodes = 20000       # Total episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b49948-ac24-4719-81c1-beac035d28f4",
   "metadata": {},
   "source": [
    "## Step 4: The Q learning algorithm üß†\n",
    "- Now we implement the Q learning algorithm:\n",
    "  ![alt text](http://simoninithomas.com/drlc/Qlearning//qtable_algo.png)\n",
    "  \n",
    "Read through the code carefully and try to figure out where each of the above steps has been implemented.\n",
    "You task is to implement:\n",
    "- step 3: Selecting the action which exploits your current Q-Table\n",
    "- step 5: Updating the Q Table using the algorithm given.\n",
    "\n",
    "You should be able to implement each of these in one line of python code. Use the hints provided to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17a47e3-cd7b-4db0-b5ac-b31e6d8b15c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (297130454.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    action = #TODO: select the action which results in the highest Q-value for the given state in the Q-table. Hint: Use np.argmax()\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped - well we'll learn only for total_episodes\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = #TODO: select the action which results in the highest Q-value for the given state in the Q-table. Hint: Use np.argmax()\n",
    "            #print(exp_exp_tradeoff, \"action\", action)\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            #print(\"action random\", action)\n",
    "                    \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # Hint: qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = # TODO: Implement the Q-Learning algorithm here! <---------------------------------------------\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145a9ef-f770-4c33-8ef2-34942a95b010",
   "metadata": {},
   "source": [
    "## Step 5: Use our Q-table to play FrozenLake ! üëæ\n",
    "- After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n",
    "- By running this cell you can see the result of our agent playing FrozenLake.\n",
    "- Note how the success rate is now much higher that when we acted randomly.\n",
    "- If your agent has successfully learned the Q-Values you should note an increase in the success rate. It should atleast be >50% on each trial\n",
    "- Experiment with the hyperparameters to understand how they impact performance.\n",
    "- Why do you think we do not get 100% success rate?\n",
    "\n",
    "- Congratulations! You have implemented your first reinforcement learning algorithm :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e9a06-ac36-42ca-a955-d9e8936ef00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qtable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPISODE \u001b[39m\u001b[38;5;124m\"\u001b[39m, episode)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     13\u001b[0m     \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Take the action (index) that have the maximum expected future reward given that state\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mqtable\u001b[49m[state,:])\n\u001b[1;32m     17\u001b[0m     new_state, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# env.render()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qtable' is not defined"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "successes = 0\n",
    "num_test_episodes = 10\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            # env.render()\n",
    "            if new_state == 15:\n",
    "                print(\"We reached our Goal üèÜ\")\n",
    "                successes += 1\n",
    "            else:\n",
    "                print(\"We fell into a hole ‚ò†Ô∏è\")\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            \n",
    "            break\n",
    "        state = new_state\n",
    "        \n",
    "print(\"Success Rate: \", (successes/num_test_episodes)*100 , \"%\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776594f3-e2be-4c1b-858a-cc0b7c2c8faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5867382-ad27-4a60-a3a7-4bc429e72cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d14a32-dda6-43a8-990a-eaf65b35d598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
