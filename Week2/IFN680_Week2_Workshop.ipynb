{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# Exploring Classical Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "Let's load in any libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc942b3a-b7e6-431b-8252-fb4dc17c9bff",
   "metadata": {},
   "source": [
    "## Part 0: Loading in the Dataset and Normalising Data\n",
    "\n",
    "We're going to be using a publicly available dataset -- the 'Maternal Health Risk Data', available from https://www.kaggle.com/datasets/csafrit2/maternal-health-risk-data\n",
    "\n",
    "From the dataset website: \"Data has been collected from different hospitals, community clinics, maternal health cares through the IoT based risk monitoring system.\n",
    "\n",
    "* Age: Age in years when a woman is pregnant.\n",
    "* SystolicBP: Upper value of Blood Pressure in mmHg, another significant attribute during pregnancy.\n",
    "* DiastolicBP: Lower value of Blood Pressure in mmHg, another significant attribute during pregnancy.\n",
    "* BS: Blood glucose levels is in terms of a molar concentration, mmol/L.\n",
    "* HeartRate: A normal resting heart rate in beats per minute.\n",
    "* Risk Level: Predicted Risk Intensity Level during pregnancy considering the previous attributes.\"\n",
    "\n",
    "We're going to see if we can predict the Risk Level of a patient -- low risk, medium risk, or high risk -- based on the other variables provided.\n",
    "\n",
    "Below, I'm going to load in the dataset and do some initial processing. There's nothing for you to change here, but I'll leave comments in case you're interested on what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b991a-873d-4cd5-a971-244c6318c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('Maternal Health Risk Data Set.csv')   #read the file into a pandas data frame\n",
    "\n",
    "print(all_data.info())   #we can call this command to get some stats on the dataset, including the features we have, the number of data points for each category, and the data type for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3125a-f5fc-460e-8cea-fa050f27d8d5",
   "metadata": {},
   "source": [
    "Above, we can see that there are 1014 data points for a variety of features. \n",
    "\n",
    "We're interested in using features 0-5 to help us predict which risk level the patient has -- 'low risk', 'medium risk', or 'high risk'.\n",
    "\n",
    "**Some important notes:** \n",
    "* we are predicting a category -- therefore this is a classification task\n",
    "* all of our input features are in a numerical form currently -- we can see this from the 'Dtype'. If any were in a categorical form, we would need to convert them first.\n",
    "\n",
    "Below, I'm going to separate the pandas dataframe into our input data, and our ground-truth output data. I'm also going to use a histogram to plot the occurence of each risk level in the dataset.\n",
    "\n",
    "Again, there's nothing for you to change here, but you should try to understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741c41f-f823-4633-b3cd-ad8c0f9b0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting both to numpy, as these will be easier to work with following on from here\n",
    "input_data = all_data[['Age', 'SystolicBP', 'DiastolicBP', 'BS', 'BodyTemp', 'HeartRate']].to_numpy()\n",
    "\n",
    "gt_output = all_data['RiskLevel'].to_numpy()\n",
    "\n",
    "plt.hist(gt_output) #plots a histogram - google if you don't know what this is\n",
    "plt.xlabel('Risk Level') #let's use good etiquette and label our graph axis\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101fdfc-dfbf-43c3-b210-b8deaecfec70",
   "metadata": {},
   "source": [
    "Looking at the histogram, some class imbalance is definitely present, but it doesn't appear major. We aren't going to change this at this point, let's just keep it in mind.\n",
    "\n",
    "Below, I'm preparing the data a little further:\n",
    "* If I needed to convert any non-numerical, categorical features into an integer, I would do that here. That's not necessary in this dataset.\n",
    "* I'm normalizing each feature so that all features are on the same scale, between 0 and 1. I'm going to use min-max scaling -- again, Google this if you're not sure what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e413f-39fb-485f-b2b7-1bb36d3443e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data sample prior to normalization:')\n",
    "print(input_data[:3])  \n",
    "\n",
    "#min-max scaling means (x-xmin)/(xmax-xmin)\n",
    "min_features = np.min(input_data, axis = 0)\n",
    "max_features = np.max(input_data, axis = 0)\n",
    "input_data = (input_data-min_features)/(max_features-min_features)\n",
    "\n",
    "print('Data sample after normalization:')\n",
    "print(input_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030bb905-7a20-4275-8c69-0b2582e68d40",
   "metadata": {},
   "source": [
    "## Part 1: Training, Validation and Test Subsets \n",
    "\n",
    "### (a) split the total dataset into a train, val and test subset\n",
    "There are many different ways to take a dataset, and split it into training, validation and test subsets. I'm going to introduce a method that will work nicely when we move on to the deep learning methods and image data.\n",
    "\n",
    "We have 1014 data points, and I've identified in the code below that I want to split this data so that we have 50% for the training subset, and 25% each for the validation and test subsets. One important consideration when splitting data -- make sure you are randomly splitting the data, so that we are likely to end up with a more balanced selection and avoid any unintended ordering in the dataset file. For example, imagine if the entries in the dataset where ordered based on the patient's age.\n",
    "\n",
    "If you're really stuck -- try googling this! **If you use code you find online, make sure you don't just copy and paste, but take time to understand how it works.** If you use code from online during the assessment, and it's wrong or you can't explain how it works, you won't get any marks for that category.\n",
    "\n",
    "Some starting points:\n",
    "* numpy split function -- https://numpy.org/doc/stable/reference/generated/numpy.split.html, just make sure that you are splitting the input_data and gt_output in the same way.\n",
    "* sklearn train_test_split function -- https://numpy.org/doc/stable/reference/generated/numpy.split.html, just be aware that this only splits the data into 2 portions. You'll need to think carefully about how to end up with train, val and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7e98b-597f-4934-a094-db344be0fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = 0.5\n",
    "val_portion = 0.25\n",
    "test_portion = 0.25\n",
    "\n",
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n",
    "# at the end of this block, you should have 6 variables created\n",
    "#            - input_train = 50% of input_data\n",
    "#            - gt_train = 50% of gt_output (should be the same 50% as was used for input_train)\n",
    "#\n",
    "#            - input_val = 25% of input_data\n",
    "#            - gt_val = 25% of gt_output (should be the same 25% as was used for input_val)\n",
    "#\n",
    "#            - input_test = 25% of input_data\n",
    "#            - gt_test = 25% of gt_output (should be the same 25% as was used for input_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef6036-ed56-4bcb-803b-93539d44fec0",
   "metadata": {},
   "source": [
    "Your total dataset should have an input shape of (1014, 6) and a ground-truth shape of (1014,).\n",
    "This is because we have 1014 data points, the input has 6 features, and the output is a single number (category of risk level).\n",
    "\n",
    "Your train, validation, and test subsets should also have 6 input features and the output as a single number, but will have different numbers of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af4ae3-4329-4038-bf14-d60ba8139936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Dataset shape:')\n",
    "print(f'    Input shape: {input_data.shape}   GT shape: {gt_output.shape}')\n",
    "print('Train Subset shape:')\n",
    "print(f'    Input shape: {input_train.shape}   GT shape: {gt_train.shape}')\n",
    "print('Validation Subset shape:')\n",
    "print(f'    Input shape: {input_val.shape}   GT shape: {gt_val.shape}')\n",
    "print('Test Subset shape:')\n",
    "print(f'    Input shape: {input_test.shape}   GT shape: {gt_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f7d9e-3678-4385-99cd-730d2d7a635c",
   "metadata": {},
   "source": [
    "### (b) visualise the class distribution across different dataset splits\n",
    "\n",
    "If you're happy with your dataset splits, let's check how the class balance looks between these splits.\n",
    "\n",
    "Use a histogram to visualise the distribution of the risk level classes in the train, validation, and test subset.\n",
    "\n",
    "If the class balance looks very different between classes for different data subsets, you may want to go back to where you split your data and fix this.\n",
    "\n",
    "Since you're using random shuffling (hopefully) and our imbalance wasn't too bad to start with, you can try re-running the cell and seeing if you get a more balanced split.\n",
    "\n",
    "Otherwise, the sklearn train_test_split function has a stratify argument that may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1606f6-e335-40a8-bd57-aea88b14719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n",
    "\n",
    "#if you need help, look above to where we visualised a histogram for gt_output\n",
    "#HINT: using the argument density = True in the plt.hist() function might help compare the shape of the distribution, despite having different absolute numbers in each subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a662c-ae6b-4d23-90d9-004069151af5",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a K Nearest Neighbour Model\n",
    "\n",
    "### (a) An Example of a K=1 Nearest Neighbour\n",
    "Below, I've shown an example of using the sklearn KNeighborsClassifier -- this uses a K Nearest Neighbour approach to classification.\n",
    "You can read from the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "Read through the below code, and understand the process that's being followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d40ec-1b66-4d0c-95a5-8f18a85925e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#initialise the ML model\n",
    "K = 1   # this is a hyperparameter\n",
    "knn_model = KNeighborsClassifier(n_neighbors=K)\n",
    "\n",
    "#fit the ML model to the training data\n",
    "knn_model.fit(input_train, gt_train)\n",
    "\n",
    "#test the ML model on the validation data\n",
    "val_pred = knn_model.predict(input_val)\n",
    "\n",
    "#Let's use the accuracy performance metric to find how good performance is on the validation data\n",
    "correct = np.sum(val_pred == gt_val)\n",
    "total = len(gt_val)\n",
    "accuracy = correct/total\n",
    "\n",
    "#Report the results\n",
    "print(f'KNN with K={K}, Accuracy of {100.*accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77443b-3014-464a-8910-7d4e73f6cdbc",
   "metadata": {},
   "source": [
    "### (b) Use the validation dataset to find K\n",
    "\n",
    "Let's use the validation dataset to find the best value of K! You can adapt the code above to search through a range of K values, store the validation accuracy, and then store the best value of K in a variable called *K_best*.\n",
    "\n",
    "It's also a good idea to plot the results you get, using something like plt.plot() -- see here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "\n",
    "Sometimes, you'll get similar performance with a high value of K and a low value of K -- remember: the lower value is usually the better choice in this case (see Occam's razor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f197a5-e52e-4c54-ae03-7c312de79475",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n",
    "\n",
    "#Report the results\n",
    "print(f'Best value of K was {K_best}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e3d05-3edc-4eb6-8296-6786c6e3a269",
   "metadata": {},
   "source": [
    "### (c) Find KNN Performance on the Test Data \n",
    "\n",
    "Now that we've used our validation dataset to find the best value of K, let's use this value of K to create a model, and then test it on the test data to see the final 'real-world' performance.\n",
    "\n",
    "Your turn -- try to implement this process. It'll be very similar to the approach from above -- you're still using a KNeighborsClassifier and fitting it to the training data subset. This time, use the K_best variable and test on the test data to find the accuracy of the model.\n",
    "\n",
    "I got KNN with K=1, and an accuracy of 74.8% -- what about you?\n",
    "\n",
    "Some differences are expected (we did use different data due to the random sampling), but your K value and test accuracy should be fairly close to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2116b8-461a-4679-88e7-1c0a3194e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "#Report the results\n",
    "print(f'KNN with K={K_best} on the test data, Accuracy of {100.*accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d96787-2920-4982-9e57-4470c5fc3b98",
   "metadata": {},
   "source": [
    "### (c) Visualise performance with a confusion matrix\n",
    "\n",
    "Create a Confusion Matrix based on the performance of the KNN model on the test dataset.\n",
    "\n",
    "Looking at the Confusion Matrix, reflect on the following questions:\n",
    "1. Is performance consistent across the classes, or is there a clear discrepancy for some classes? If there is, why do you think this might be?\n",
    "2. Given the potential use of this ML model, are some types of errors worse or more dangerous than others? How does the KNN model perform for these types of errors? (e.g. if a patient is medium risk, is it better or worse for them to be misclassified as low risk or high risk?)\n",
    "\n",
    "Sklearn has a useful function -- ConfusionMatrixDisplay.from_predictions() -- that creates a confusion matrix if given an array of predicted labels and an array of true labels. Read the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions\n",
    "\n",
    "Note: you may want to use the normalize argument in the above function to allow easy interpretation in the presence of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609ecd0-1579-4e3a-94b7-18b27a2a2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77209788-bcb1-4f40-9722-722cc806ea0c",
   "metadata": {},
   "source": [
    "## Part 3: Implementing a Random Forest Classifier\n",
    "\n",
    "### (a) Use the validation dataset to find the best number of estimators\n",
    "In the cell below, implement the sklearn RandomForestClassifier and find the best value for the number of trees in the forest using the validation dataset. Adding more trees usually offers more robust performance, but it also comes at the cost of slower performance. You will likely follow these steps:\n",
    "* Read the sklearn documentation on RandomForestClassifier to see how to implement -- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* For each number in the num_trees list:\n",
    "    * initialise a RandomForestClassifier with that number of trees\n",
    "    * fit the classifier to the training data\n",
    "    * find accuracy of the classifier on the validation dataset\n",
    "    * record this accuracy and the number of trees currently being tested\n",
    "* Create a plot to visualise the number of trees versus the accuracy, and choose the best value for the number of trees.\n",
    "    * to pick the best number of trees, you'll need to weigh up speed versus accuracy.\n",
    "    * Remember with Random Forest that the results are not deterministic and can change each time you fit the classifier. You can run this cell a number of times to get a sense of consistency of performance\n",
    "    * You can also use the time.time() function to get an idea of run-time for each value -- see https://www.tutorialspoint.com/python/time_time.htm\n",
    "\n",
    "*Note: the cell will take about 1-2 minutes to run through all values in est_range*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7dc414-5e71-426e-a414-9736b49cdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "num_trees = [i for i in range(50, 1500, 50)] #check from 50-1500 in increments of 50\n",
    "\n",
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff479d9-471e-47de-acd0-4d83778bc638",
   "metadata": {},
   "source": [
    "### (b) Find the performance on the test dataset with your selected best number of estimators\n",
    "\n",
    "Fill in the best_trees variable with your chosen number of trees. I opted for best_trees = 250, as this seemed to perform pretty consistently when I tested and also didn't have too much computational cost.\n",
    "\n",
    "Then write the code to (1) create a classifier with best_trees number of trees, (2) fit on the training dataset, (3) test on the test dataset and (4) report accuracy.\n",
    "\n",
    "My accuracy was ~77.95%, better than the KNN model! Did you get the same?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d0c5e-7fba-419f-b647-c9c156617bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n",
    "\n",
    "best_trees = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11e7650-e0fe-4623-81dd-4d6cdba7f4e4",
   "metadata": {},
   "source": [
    "### (c) Visualise performance with a confusion matrix\n",
    "\n",
    "Use the same approach as earlier.\n",
    "\n",
    "How does performance compare with the KNN classifier? Does it have a similar distribution of errors, or different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006ffe8-1bad-4d48-83c4-5f776c612b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################################################################\n",
    "################################################     YOUR CODE GOES BELOW     ####################################################################################################################################\n",
    "##################################################################################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8772e21-2603-4363-bb2a-e05fe38e8fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
