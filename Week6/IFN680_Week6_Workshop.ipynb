{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# Performance Metrics and Calibration Curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a3d7b-dfbd-4bc8-acb8-00c95b035be2",
   "metadata": {},
   "source": [
    "In this week, we're going to evaluate our semi-supervised model from Week 5 and take a deeper look at how it performs for each class, as well as the calibration of the confidence it produces.\n",
    "\n",
    "To do this, we're going to follow these steps:\n",
    "1. Initialise the notebook by loading necessary libraries.\n",
    "2. Load the test data for evaluation. \n",
    "3. Initialise our student network and load the pretrained weights from last week.\n",
    "4. Test the student network on the test data, collecting all predictions, the associated confidence, and the ground-truth class.\n",
    "5. Create a confidence calibration curve.\n",
    "6. Investigate the precision and recall of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "# 1) Initialise the notebook with libraries\n",
    "\n",
    "Let's load in any libraries we will use in this notebook. We're going to install weights and biases (wandb) as it does not come by default in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743d496-b334-499f-b57f-da86a441f925",
   "metadata": {},
   "source": [
    "# 2) Load the labelled test data and prepare for evaluation\n",
    "\n",
    "## 2a) Load the test dataset\n",
    "\n",
    "Last week, we saw that we can load data with this format by:\n",
    "1. Applying transformations -- by default, the most basic is [transforms.ToTensor()](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html), [transforms.Resize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) and [transforms.Normalize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html).\n",
    "2. Load the datasets in with [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html).\n",
    "\n",
    "**If this is unfamiliar, please review the Week 4 practical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14c9ab-3029-4195-932f-af7ee5666c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias = True), \n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder('../Week 5/stanford_dogs_semi-supervised/labelled/Test/', transform = transform)\n",
    "\n",
    "class_labels = test_dataset.classes\n",
    "num_classes = len(class_labels)\n",
    "\n",
    "print(f'Dataset has {num_classes} classes, which are: {class_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a60f3-e1ef-4571-bfa9-5fd4588477d5",
   "metadata": {},
   "source": [
    "## 2b) Initialise the data loaders\n",
    "\n",
    "**Your turn:** Using a batch size of 8, initialise the data loaders for the ```test_dataset``` below into a variable called ```testloader```.\n",
    "\n",
    "If you are confused, you can check the Week 3 and Week 4 practical sheets or review the IFN680 practical support sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6313c-54ad-4996-904e-f3bf9d860de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### Your code goes here ################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b7ba5-55b5-4350-abe2-9eb88cc38537",
   "metadata": {},
   "source": [
    "# 3) Initialise and load weights for our student network\n",
    "\n",
    "We can use the ```create_classifier``` function from last week's practical to create our network. We can then load in our existing weights for this model.\n",
    "\n",
    "If this is unfamiliar, you can review the Week 4 and Week 5 practical sheets or read more in the IFN680 Practical Support Sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2943b5-399d-4e6b-970b-909859cd440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(nc):\n",
    "    #load the model and initialise with pre-trained weights\n",
    "    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    #adapt the architecture to the correct number of classes\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(model.fc.in_features, nc)\n",
    "    \n",
    "    #move the model to the GPU\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_classifier(num_classes)\n",
    "model.load_state_dict(torch.load('Week5_ResNet_student_best.pth')) #I am loading in my weights from the Week 5 practical -- you can change this to your own if you'd like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886e3de-c67d-4dbd-8545-f686b7bb4b6e",
   "metadata": {},
   "source": [
    "# 4) Test the student network on the test data, collecting all predictions, the associated confidence, and the ground-truth class.\n",
    "\n",
    "This will follow a very similar process to what we typically do when testing on the validation dataset during training.\n",
    "\n",
    "One thing that we need to do differently here is normalize our class scores. The ResNet architecture will output unnormalised scores, that haven't yet gone through a Softmax layer. We can remedy this by applying the [torch.softmax()](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax) function to the output from the model (see the IFN680 support material for more information). Please review the Week 3 lecture slides (slide 51) and the IFN680 support material for more information on the Softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ffd68-bf65-4de2-a8f9-2e8c80c6f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "model.eval()\n",
    "\n",
    "all_pred_conf = []\n",
    "all_pred_class = []\n",
    "all_gt_class = []\n",
    "\n",
    "for i, data in  tqdm.tqdm(enumerate(testloader, 0), total = len(testloader)):\n",
    "    inputs, labels = data\n",
    "\n",
    "    #A. move the inputs to the GPU if available\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    #B.  forward pass to find the outputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    #use the softmax function to convert the class scores into normalized confidence scores\n",
    "    outputs = torch.softmax(outputs, dim = 1)\n",
    "    \n",
    "    #we need to know which label is predicted by our model. This is the class with the highest class score.\n",
    "    predicted_class = torch.argmax(outputs, axis = 1)\n",
    "    \n",
    "    #we also need to know the confidence predicted  by our model\n",
    "    predicted_confidence, _ = torch.max(outputs, axis = 1)\n",
    "    \n",
    "    #convert all our important information to list format, and store in the respective lists for processing later\n",
    "    all_pred_conf += predicted_confidence.cpu().tolist()\n",
    "    all_pred_class += predicted_class.cpu().tolist()\n",
    "    all_gt_class += labels.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84a11b-076e-41f1-8e85-462e286f02f0",
   "metadata": {},
   "source": [
    "# 5) Create a confidence calibration curve.\n",
    "\n",
    "As discussed in the Week 6 lecture, calibration curves are useful for understanding how well the confidence scores predicted by a classification model align with the actual accuracy of the model, which is critical in some applications where not just the label but also the uncertainty of the prediction is important. A well-calibrated model should have its predicted confidence probabilities close to the true probability that the prediction is correct.\n",
    "\n",
    "Below, we're creating a calibration curve which tests confidence calibration at 10 intervals between 0-100%. Please see the Week 6 lecture slides for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45e776-4845-47af-9be4-1b6c5cbe7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable that holds the confidence intervals we will check on a confidence calibration curve\n",
    "conf_ranges = [[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100]] \n",
    "\n",
    "#convert our previously collected lists into numpy arrays so that we can easily manipulate them\n",
    "all_pred_conf = np.array(all_pred_conf)\n",
    "all_pred_class = np.array(all_pred_class)\n",
    "all_gt_class = np.array(all_gt_class)\n",
    "\n",
    "actual_accuracy = []\n",
    "conf_level = []\n",
    "for conf_int in conf_ranges:\n",
    "    lower = conf_int[0]/100 #convert between 0-1\n",
    "    upper = conf_int[1]/100 #convert between 0-1\n",
    "\n",
    "    #create a mask that will collect predictions in the confidence interval -- it must be above the lower thresh AND below the upper thresh\n",
    "    mask = (all_pred_conf >= lower) & (all_pred_conf < upper)\n",
    "    \n",
    "    #collect all predictions and GT data within the range using the mask\n",
    "    preds = all_pred_class[mask]\n",
    "    gt = all_gt_class[mask]\n",
    "    \n",
    "    #find the accuracy of this bin by checking how many correct/total\n",
    "    correct = np.sum(preds == gt)\n",
    "    total = len(preds)\n",
    "    accuracy = correct/total\n",
    "    actual_accuracy += [accuracy] #save the accuracy for this bin to plot later\n",
    "    conf_level += [(upper + lower)/2] #this is the average confidence level for this confidence interval (not necessarily for the predictions in the bin though), we will use this for plotting later\n",
    "    \n",
    "plt.figure()\n",
    "plt.bar(conf_level, actual_accuracy, width = 0.09)\n",
    "plt.plot([0, 1], [0, 1], 'r--') #our well-calibrated line\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Confidence Calibration Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524376a-3521-43c1-8774-80c90b7847e1",
   "metadata": {},
   "source": [
    "## Consider:\n",
    "\n",
    "Would you consider this model to be well-calibrated? If not, is it over-confident or under-confident? What's going on for confidences below 0.3?\n",
    "\n",
    "**Your turn:** Add some code to the above cell to try to create another plot that shows the number of data points that were used to calculate the accuracy of each bin. This may give you more information about how reliable certain parts of the confidence calibration curve is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9c770-666e-42b7-835d-e04d8e0963e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Your code goes below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94069ade-92d3-4a77-aa94-ffc769b94b0c",
   "metadata": {},
   "source": [
    "# 6) Investigate the precision and recall of each class.\n",
    "\n",
    "To calculate precision and recall for each class, we need to find the number of true positives for each class (actual examples from that class that were correctly classified), false positives (examples not belonging to that class that were classified as the class incorrectly), and false negatives (actual examples from that class that were incorrectly classified as a different class).I've done that below. \n",
    "\n",
    "**Your turn:** Use the TP, FP and FN counts to calculate the precision and recall for the class. Review the Week 6 lecture notes for the precision and recall formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6333b2a6-109d-4b70-81a7-3d505e8b7556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cls_idx in range(num_classes):\n",
    "    \n",
    "    mask_gt_pos = all_gt_class == cls_idx #mask for which samples belong to the current positive class?\n",
    "    mask_pred_pos = all_pred_class == cls_idx #mask for which samples were predicted as the current positive class?\n",
    "    \n",
    "    tp_mask = mask_gt_pos & mask_pred_pos #belongs to the pos class AND was predicted as the pos class\n",
    "    tp_count = np.sum(tp_mask) #the count of TP for this class\n",
    "    \n",
    "    fp_mask = ~mask_gt_pos & mask_pred_pos #does not belong to the pos class AND was predicted as the pos class -- note the use of ~to find the inverse mask\n",
    "    fp_count = np.sum(fp_mask)#the count of FP for this class\n",
    "    \n",
    "    fn_mask = mask_gt_pos & ~mask_pred_pos #belongs to the pos class AND was not predicted as the pos class -- note the use of ~to find the inverse mask \n",
    "    fn_count = np.sum(fn_mask)#the count of FN for this class\n",
    "    \n",
    "    ### Your code goes below\n",
    "    precision = ...\n",
    "    recall = ...\n",
    "    \n",
    "    \n",
    "    print(f'For class {class_labels[cls_idx]}: ')\n",
    "    print(f'              Precision: {precision}')\n",
    "    print(f'              Recall: {recall}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf975f4-9306-43a1-b24a-fd1f93146278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
