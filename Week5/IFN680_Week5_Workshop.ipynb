{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# Semi-supervised learning using student-teacher techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a3d7b-dfbd-4bc8-acb8-00c95b035be2",
   "metadata": {},
   "source": [
    "In this week, we're going to use student-teacher techniques for semi-supervised learning. \n",
    "\n",
    "As a reminder from the Week 5 lecture content:\n",
    "**Semi-supervised learning** uses both labelled and unlabelled data for training, often leading to better model performance than using labelled data alone, particularly when there is small quantities of labelled data.\n",
    "In the context of semi-supervised learning, **student-teacher techniques** involves using a \"teacher\" model (trained on the labelled data) to generate pseudo-labels for unlabelled data, which then guides the training of a \"student\" model. You can see an illustration of this below.\n",
    "\n",
    "![Student-teacher techniques](images/Student-teacher-Training.jpg)\n",
    "\n",
    "To do this, we're going to follow these steps:\n",
    "1. Initialise the notebook by loading necessary libraries.\n",
    "2. Load the labelled data and prepare for training.\n",
    "3. Initialise and train a teacher network on the labelled data.\n",
    "3. Load the unlabelled data.\n",
    "4. Use the teacher network to pseudo-label the unlabelled dataset.\n",
    "5. Combine the pseudo-labelled data and labelled data into one dataset for training.\n",
    "5. Train a student network on both the labelled and pseudo-labelled data. (Pseudo-labelling self-supervision technique)\n",
    "\n",
    "If you finish this practical early, you could also further investigate the questions below (some resources are provided for (1), and some food for thought provided for (2) and (3) if you're interested in this).\n",
    "1. Can we use noisy student-teacher learning? Does this improve performance?\n",
    "2. Can you more carefully select which pseudo-labels to use? Perhaps by checking the confidence scores?\n",
    "2. Can we do multiple iterations of student-teacher learning to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "# 1) Initialise the notebook with libraries\n",
    "\n",
    "Let's load in any libraries we will use in this notebook. We're going to install weights and biases (wandb) as it does not come by default in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import tqdm\n",
    "\n",
    "!pip install wandb\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743d496-b334-499f-b57f-da86a441f925",
   "metadata": {},
   "source": [
    "# 2) Load the labelled data and prepare for training\n",
    "## 2a) Inspect the Data\n",
    "**Make sure you've extracted the dataset folder by right-clicking and selecting 'Extract Archive'**. Once you've done this, look at the different folders and how the dataset is structured. Click open some images to see what they look like and get a feel for the data you're going to be working with.\n",
    "\n",
    "As you can see, we have the following structure:\n",
    "```\n",
    "└── stanford_dogs_semi-supervised\n",
    "    ├── labelled                     \n",
    "    |   ├── Train                     # training and validation data folder, with classes separated into sub-directories\n",
    "    |   |   ├── Class 1\n",
    "    |   |   ├── Class 2\n",
    "    |   |   └── ....\n",
    "    |   └── Test                      # test data folder, with classes separated into their own folders as sub-directories\n",
    "    |       ├── Class 1\n",
    "    |       ├── Class 2\n",
    "    |       └── ....\n",
    "    └── unlabelled                     #approx. 1000 images, not sorted into class folders\n",
    "```\n",
    "## 2b) Loading the Labelled Data\n",
    "\n",
    "Last week, we saw that we can load data with this format by:\n",
    "1. Applying transformations -- by default, the most basic is [transforms.ToTensor()](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html), [transforms.Resize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) and [transforms.Normalize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html).\n",
    "2. Load the datasets in with [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html).\n",
    "\n",
    "**If this is unfamiliar, please review the Week 4 practical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14c9ab-3029-4195-932f-af7ee5666c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias = True), \n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainval_dataset = torchvision.datasets.ImageFolder('stanford_dogs_semi-supervised/labelled/Train/', transform = transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('stanford_dogs_semi-supervised/labelled/Test/', transform = transform)\n",
    "\n",
    "class_labels = trainval_dataset.classes\n",
    "num_classes = len(class_labels)\n",
    "\n",
    "print(f'Dataset has {num_classes} classes, which are: {class_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b12388-1c03-4438-a16e-4b8dcb15b85e",
   "metadata": {},
   "source": [
    "## 2c) Split the Labelled Data and Visualise\n",
    "\n",
    "## **IMPORTANT NEW INFORMATION**\n",
    "\n",
    "Previously, we've used random functions to split our data into training and validation data splits. This randomness is good, but it also means that every time we run it we may get something different. This could be problematic if you save a model from some initial tests, then come back to the workbook and run more tests and re-split the training/validation dataset, because you might get data leakage (training and validation data get mixed up).\n",
    "\n",
    "We can set the \"random\" seed for the libraries where we use random functions. In JupyterLab, setting the random seed ensures reproducibility across different runs of the entire notebook; this means that every time you restart and run the notebook from the beginning, you will get the same outcomes for operations involving randomness. However, within a single notebook session, if you repeatedly run a function that involves randomness without resetting the seed in each call, the outcomes can vary.\n",
    "\n",
    "**Your turn:** Use [torch.utils.data.random_split()](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to randomly separate the ```trainval_dataset``` into a training and validation subset. \n",
    "\n",
    "You should have completed this in the Week 3 and 4 practical. You can copy and paste that here, or if you're stuck there is extra support in the IFN680 Practical Support Sheet.\n",
    "\n",
    "Once you've done this, we'll visualise the data and the split of classes between the training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843dbdf-8cc5-43e8-83e4-3457eafd2d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1) # this is important to use every time you re-run the random_split function-- it means you'll continually get the same random split of the training and validation dataset\n",
    "\n",
    "train_portion = 0.6\n",
    "\n",
    "##### Your code goes here ######\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "print(f'Size of train dataset: {len(train_dataset)}')\n",
    "print(f'Size of val dataset: {len(val_dataset)}')\n",
    "print(f'Size of test dataset: {len(test_dataset)}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 5)\n",
    "for idx in range(5):\n",
    "    train_image = (train_dataset[idx][0].numpy())/2 + 0.5\n",
    "    label = class_labels[train_dataset[idx][1]]\n",
    "    train_image = np.moveaxis(train_image, 0, 2)\n",
    "    ax[idx].imshow(train_image)\n",
    "    ax[idx].set_axis_off()\n",
    "    ax[idx].set_title(label.split('-')[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#### Your code goes below to create the histogram of class distributions between the training and validation data subsets\n",
    "train_labels = [data[1] for data in train_dataset]\n",
    "val_labels = [data[1] for data in val_dataset]\n",
    "\n",
    "plt.hist([train_labels, val_labels], label = ['Train', 'Val'], density = True) \n",
    "plt.xlabel('Class Label')\n",
    "plt.xticks([i for i in range(num_classes)], class_labels, rotation=90)\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a60f3-e1ef-4571-bfa9-5fd4588477d5",
   "metadata": {},
   "source": [
    "## 2d) Initialise the data loaders\n",
    "\n",
    "**Your turn:** Using a batch size of 8, initialise the data loaders for the ```train_dataset```, ```val_dataset```, and ```test_dataset``` below. The training data loader should shuffle the data. Use a single data worker per dataloader. \n",
    "\n",
    "If you are confused, you can check the Week 3 and Week 4 practical sheets or review the IFN680 practical support sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6313c-54ad-4996-904e-f3bf9d860de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### Your code goes here ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b7ba5-55b5-4350-abe2-9eb88cc38537",
   "metadata": {},
   "source": [
    "# 3) Initialise and train a teacher network on the labelled data\n",
    "\n",
    "## 3a) Initialise the teacher model, loss function and optimiser\n",
    "Now that we've inspected the data and done our initial pre-processing, we need to initialise some other important things before we can begin training. These include:\n",
    "1. Initialise the teacher model we will use for classification, including intialising the weights with the pre-trained ImageNet weights.\n",
    "2. Adapt the model for transfer learning.\n",
    "3. Move the model to the GPU if it is available.\n",
    "4. Initialise the optimiser (stochastic gradient descent) that will update parameters for us.\n",
    "5. Initialise the loss function we will use to supervise the training of our model.\n",
    "\n",
    "These should all be familiar from the last practical. Some of these are easy to do and don't need to be changed during the training process, i.e. the loss function initialisation. Other things, i.e. the model initialisation and optimiser initialisation, might need to be flexible as we iteratively test over and over. We're now going to create a number of functions that do these things, so that we can constantly call these functions without having to copy and paste code.\n",
    "\n",
    "### Function 1: Create the model, ready for training\n",
    "Assumed input: the number of classes of our new dataset.\n",
    "1. Initialise the model we will use for classification, including intialising the weights with the pre-trained ImageNet weights.\n",
    "2. Adapt the model for transfer learning.\n",
    "3. Move the model to the GPU if it is available.\n",
    "Assumed output: the model\n",
    "\n",
    "### Function 2: Initialise an optimiser with certain parameters\n",
    "Assumed input: the model, learning rate and momentum parameters for the optimiser.\n",
    "1. Initialise the optimiser (stochastic gradient descent) that will update parameters for us.\n",
    "Assumed output: an optimiser with the given parameters.\n",
    "\n",
    "If this is unfamiliar, you can review the Week 4 practical sheet or read more in the IFN680 Practical Support Sheet.\n",
    "\n",
    "**Your turn:** I have completed Function 1. Try adapting the code to create function 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2943b5-399d-4e6b-970b-909859cd440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(nc):\n",
    "    #load the model and initialise with pre-trained weights\n",
    "    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    #adapt the architecture to the correct number of classes\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(model.fc.in_features, nc)\n",
    "    \n",
    "    #move the model to the GPU\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#this is fairly simple code -- we don't need to re-initialise this, so I won't put it into a function.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "######################### Your code goes below #######################################  \n",
    "def init_optimiser():\n",
    "    #complete this function, including the assumed input arguments\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1584da-7f66-47fb-a65f-8d8fa122975b",
   "metadata": {},
   "source": [
    "## 3b) Train the teacher model on the training dataset\n",
    "\n",
    "There is a cell below that will train the teacher model on the training dataset. \n",
    "It follows these steps:\n",
    "1. Set the hyperparameters for training -- hyperparameters relating to the optimizer, or any other part of the training process.\n",
    "2. Initialise the model and optimizer using our new functions.\n",
    "2. Initialise the visualisation tool \n",
    "We recommend using ```wandb```. If this is unfamiliar, review the Week 4 practical sheet or read more in the IFN680 practical support sheet.\n",
    "3. For the given number of epochs\n",
    "    1. Train on the training dataset, measuring accuracy and loss over the whole dataset.\n",
    "    2. Measure performance on the validation dataset, measuring accuracy and loss over the whole dataset.\n",
    "    3. If the validation accuracy is higher than it previously was, save these weights as the 'best' weights.\n",
    "    4. Log the performance frmo the training and validation dataset.\n",
    "    \n",
    "**Your turn:** Try to adapt the parameters so that you create a teacher model that converges during training. I was able to achieve a validation accuracy of 89% - aim for this as a sign that the model has converged. You should try changing the learning rate and number of epochs to achieve this.   \n",
    "*Hint: Remember that this will be training the model on a very small amount of data.. we're trying to use fine-tuning here, so if you observe poor performance, it's probably because your learning rate is too high and you're \"training away\" the good features learnt from ImageNet.*\n",
    "\n",
    "If any part of this is unclear, you should review the Week 4 practical sheet and IFN680 Practical Support Sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ffd68-bf65-4de2-a8f9-2e8c80c6f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters for training\n",
    "lr = 0.01 # learning rate for SGD - 0.01\n",
    "momentum = 0.9 # momentum for SGD\n",
    "num_epochs = 5 # how many times to train over entire training dataset - 5\n",
    "\n",
    "#create the teacher model ResNet18, ready for fine-tuning, with the new function\n",
    "teacher_model = create_classifier(num_classes)\n",
    "\n",
    "#create the optimiser\n",
    "optimizer = init_optimiser(teacher_model, lr, momentum)\n",
    "\n",
    "\n",
    "#Initialise wandb\n",
    "wandb.login()\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"momentum\": momentum,\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": num_epochs\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"Week 5 practical worksheet\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=f\"semisupervised_teacher_{lr}_{num_epochs}\", \n",
    "    # Track hyperparameters and run metadata\n",
    "    config=config, reinit = True)\n",
    "\n",
    "\n",
    "best_accuracy = 0 # this is the running best accuracy on the validation dataset\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "\n",
    "for epoch in range(num_epochs):    \n",
    "    \n",
    "    teacher_model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, data in  tqdm.tqdm(enumerate(trainloader, 0), total = len(trainloader), desc = f'Epoch {epoch+1} - training phase'):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #A. move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #B. zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #C.  forward pass to find the outputs\n",
    "        outputs = teacher_model(inputs)\n",
    "        \n",
    "        #D. calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #E. backward pass to calculate the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #F. take a step with gradient descent to change the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #G. let's keep track of the loss and the accuracy\n",
    "        train_loss += [loss.cpu().item()]\n",
    "        \n",
    "        # To find the accuracy, we need to know which label is predicted by our model. This is the class with the highest class score.\n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        \n",
    "        #now we'll count how many were correct vs how many there were - we can use this later to find the total accuracy of the epoch\n",
    "        train_correct += torch.sum(predicted == labels).cpu().item()\n",
    "        train_total += len(labels)\n",
    "\n",
    "    #3. record the mean loss and accuracy over the entire epoch and training dataset\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    train_accuracy = train_correct/train_total\n",
    "    \n",
    "    #log with wandb\n",
    "    wandb.log({\"training_loss\": mean_train_loss, \"training_accuracy\": train_accuracy}, step = epoch)\n",
    "    \n",
    "    ### VALIDATION PHASE\n",
    "        \n",
    "    teacher_model.eval()\n",
    "    \n",
    "    val_loss = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    for i, data in  tqdm.tqdm(enumerate(valloader, 0), total = len(valloader), desc = f'Epoch {epoch+1} - validation phase'):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #A. move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #B.  forward pass to find the outputs\n",
    "        outputs = teacher_model(inputs)\n",
    "        \n",
    "        #C. calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #D. let's keep track of the loss and the accuracy\n",
    "        val_loss += [loss.cpu().item()]\n",
    "        \n",
    "        # To find the accuracy, we need to know which label is predicted by our model. This is the class with the highest class score.\n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        \n",
    "        #now we'll count how many were correct vs how many there were - we can use this later to find the total accuracy of the epoch\n",
    "        val_correct += torch.sum(predicted == labels).cpu().item()\n",
    "        \n",
    "        val_total += len(labels)\n",
    "\n",
    "    #3. record the mean loss and accuracy over the entire epoch and training dataset\n",
    "    mean_val_loss = np.mean(val_loss)\n",
    "    val_accuracy = val_correct/val_total\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "        torch.save(teacher_model.state_dict(), 'Week5_ResNet_teacher_best.pth')\n",
    "        best_accuracy = val_accuracy\n",
    "        print(f'Model saved for val accuracy {best_accuracy*100}% at epoch {epoch}')\n",
    "    \n",
    "    #log with wandb\n",
    "    wandb.log({\"val_loss\": mean_val_loss, \"val_accuracy\": val_accuracy}, step = epoch)\n",
    "    \n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150b407-0a91-435b-8faa-c7b7cc1689a7",
   "metadata": {},
   "source": [
    "# 4) Load the unlabelled data.\n",
    "\n",
    "In our dataset, we have an ```unlabelled``` folder that is filled with images with no class label. We can't use the standard [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) function to create a torchvision dataset in this case.\n",
    "\n",
    "Instead, we must create a custom torch dataset. You can read through the below to understand what is going on, or look in the IFN680 Practical Support Sheet for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb376c3-f00b-42ee-a3a6-b86c2ef10c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnlabelledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, im_dir, transform):\n",
    "        self.im_paths = glob.glob(im_dir) #this creates a list of all the images in im_dir\n",
    "        self.transform = transform #this is the transform we want to apply to all images -- this should be the same as in the training dataset.\n",
    "        self.targets = len(self.im_paths)*[-1] #initialise as invalid value (-1) because we have no labels at this point, but we want to populate this with pseudolabels\n",
    "        \n",
    "    def __len__(self):\n",
    "        #custom datasets need a __len__ function to return the number of data points\n",
    "        return len(self.im_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #custom datasets need a __getitem__ function to return the data point and label at a given index\n",
    "        \n",
    "        im_path = self.im_paths[idx] #find the directory of image at idx\n",
    "        image = Image.open(im_path) #use the PIL.Image.open function to open image as a PIL.Image\n",
    "        \n",
    "        transformed_image = self.transform(image) #apply the relevant transformation\n",
    "        \n",
    "        label = self.targets[idx] #grab the classification label of image at idx\n",
    "        \n",
    "        return transformed_image, label #return the transformed image and it's respective label\n",
    "    \n",
    "unlabelled_dataset = UnlabelledDataset('stanford_dogs_semi-supervised/unlabelled/*', transform)  #create the dataset using images in the unlabelled folder and our initial transform for resizing and normalizing images.s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0aa0e-4436-4f95-8344-409a50554647",
   "metadata": {},
   "source": [
    "# 5) Use the teacher network to pseudo-label the unlabelled dataset.\n",
    "\n",
    "## 5a) Load the best weights into the teacher model\n",
    "In the cell below, add code to load the ```Week5_ResNet_teacher_best.pth``` weights into the ```teacher_model```. You can review the Week 4 practical sheet for how to do this, or check the IFN680 Practical Support Sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09863447-c5ef-4368-bb09-70b9802d82b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Your code goes below ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a01e7-9182-4416-bf0d-ae945d97331d",
   "metadata": {},
   "source": [
    "## 5b) Test the teacher model on the unlabelled dataset\n",
    "In the cell below, we're going through the unlabelled dataset and finding the predicted class label with the ```teacher_model```.\n",
    "\n",
    "**Your turn:** Change the class label (held in the ```targets``` field) for the data point at ```idx``` in the ```unlabelled_dataset``` to be equal to the ```predicted``` class label from the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99222980-999c-46e6-a47e-4287f83eaaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher_model.eval()\n",
    "\n",
    "for idx in range(len(unlabelled_dataset)):\n",
    "    im, lbl = unlabelled_dataset[idx] #grab the data at idx\n",
    "    im = im.to(device).unsqueeze(0) #the model expects a batch of images, so we add another dimension to the image to create a batch with 1 image\n",
    "\n",
    "    output = teacher_model(im).detach().cpu().numpy() # we need to detach the output from the gradient calculator in PyTorch, and move it back to the CPU, then convert to a numpy array\n",
    "      \n",
    "    # To choose a pseudo-label, we need to know which label is predicted by our model. This is the class with the highest class score.\n",
    "    predicted = np.argmax(output)\n",
    "    \n",
    "    #Enter your code below, to change the dataset label at idx to the predicted class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc75a0-b97f-4b88-a643-0b661e8740ad",
   "metadata": {},
   "source": [
    "## 5b) Visualise the distribution of pseudo-labels\n",
    "\n",
    "It's worth checking what our teacher model predicted on the unlabelled data -- how many times did it predict each class? How does this relate to the distribution of classes in the training dataset?\n",
    "\n",
    "There's no correct answer here, but some times we can observe weird biases towards certain classes at this stage.\n",
    "\n",
    "**Your turn:** Below, create a histogram of the ```unlabelled_dataset.targets``` and look for trends in the class predictions. If you're going well for time, compare this to the labels in the ```train_dataset```. Is there a big change in distribution between the training labels and pseudo-labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef4bb0-97e5-4b6f-b971-29e83e867f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3128206-2170-4ee2-bbd3-d2184ea682ce",
   "metadata": {},
   "source": [
    "# 6) Combine the pseudo-labelled data and labelled data into one dataset for training.\n",
    "\n",
    "Now that we have our pseudo-labelled ```unlabelled_dataset```, we can use the [torch.utils.data.ConcatDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.ConcatDataset) to combine this with our original ```train_dataset```.\n",
    "\n",
    "**Your turn:** Load the newly created ```student_train_dataset``` into a dataloader, using batch size of 8 and shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79144269-a903-41d3-b212-02750b32d36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "student_train_dataset = torch.utils.data.ConcatDataset([unlabelled_dataset, train_dataset])\n",
    "\n",
    "#### Enter your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c46171-3ff8-49fa-8be7-cac07220074d",
   "metadata": {},
   "source": [
    "## 7) Train a student network on both the labelled and pseudo-labelled data.\n",
    "\n",
    "**Your turn:** Use the code developed in Step 3b to train the student network. See what validation accuracy you can achieve -- I could get to 91.8%, this is nearly 3% better than the teacher model. Make sure you choose a sensible learning rate and number of epochs to train for.\n",
    "\n",
    "## **IMPORTANT INFORMATION**\n",
    "It is very important that if you copy and paste code, you change the following variables:\n",
    "1. Rather than creating a ```teacher_model```, you should create a ```student_model```\n",
    "2. Everywhere that had ```teacher_model``` should be trained to your newly created ```student_model```.\n",
    "**This is seriously important, I recommend using ctrl+f to check you haven't missed any.**\n",
    "3. Update your ```wandb.init()``` to show that you are training the student model.\n",
    "4. Use the ```student_trainloader```, not the trainloader.\n",
    "5. Update the section that saves the weights to use a filename that includes \"student\", not \"teacher\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0988c7-f78e-43c9-8aa7-78ba9d88074a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Enter the code for student training here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d78a92-614c-401c-a907-90a5274a54a6",
   "metadata": {},
   "source": [
    "# Next Step (if you have time): Noisy student-teacher training \n",
    "With the above setup, our student model is always going to be somewhat limited by the teacher model -- it will learn the teacher models mistakes, and probably learn the knowledge in a similar way. This means we can get performance that is generally at least equal, and somehwat better to the teacher model, but it's hard to get much better.\n",
    " \n",
    "To overcome this, we can use noisy student-teacher training. Noisy student-teacher training introduces noise into the student model during training, enhancing its robustness and generalization by learning from both labeled data and the teacher's pseudo-labeled data. One way to introduce noise is by adding random data augmentations! You can see a refresher on some random data augmentations below. As a start, try using my ```noise_transform``` below and seeing if it improves performance when you add it into the above student training loop. Make sure to add the noise_transform to transform the data in the training loop, and to change the name of your run to include ```noisystudent```. I found that RandomHorizontalFlip() could achieve an accuracy of 91.4%, very similar to the standard student network result, so likely not quite enough noise to make noisy student training helpful. If this works, try adding more noisy transforms to see how you can optimise performance.\n",
    "\n",
    "1. [transforms.RandomResizedCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomResizedCrop.html) -- this function randomly grabs a portion of the image (crops) and then resizes to the desired image size. By default, the crop can be anywhere between 8% to 100% of the image original area.\n",
    "2. (already implemented below) [transforms.RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html) -- randomly flips an image horizontally. Useful for tasks where horizontal orientation doesn't change the meaning.\n",
    "3. [transforms.RandomVerticalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomVerticalFlip.html) -- randomly flips an image vertically. Useful for tasks where vertical orientation doesn't change the meaning.\n",
    "4. [transforms.RandomRotation](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomRotation.html) -- randomly rotates an image by a specified angle. Can simulate variations in viewpoint.\n",
    "5. [transforms.ColorJitter](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) -- randomly changes brightness, contrast, saturation, and hue of an image. Helps the model to be robust to different lighting conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa96a7-0bcf-4d9c-a79a-913d1d02b292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "    ])\n",
    "\n",
    "\n",
    "#visualise the train dataset with these transforms\n",
    "data = next(iter(trainloader))\n",
    "fig, ax = plt.subplots(1, 5)\n",
    "for idx in range(5):\n",
    "    im = data[0][idx]\n",
    "    lbl = data[1][idx]\n",
    "    im = noise_transform(im)\n",
    "    train_image = (im.numpy())/2 + 0.5\n",
    "    label = class_labels[lbl]\n",
    "    train_image = np.moveaxis(train_image, 0, 2)\n",
    "    ax[idx].imshow(train_image)\n",
    "    ax[idx].set_axis_off()\n",
    "    ax[idx].set_title(label.split('-')[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98978809-cba8-4356-b90b-e82447d1f53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Enter the code for noisy student training here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067875ba-b0bb-47f5-8f21-dbee6cb3276a",
   "metadata": {},
   "source": [
    "# Further food for thought\n",
    "\n",
    "## Improving the quality of pseudo-labels\n",
    "In the lecture, we discussed how thresholding the confidence of pseudo-labels in can help ensure that the model is trained on high-quality, likely correct pseudo-labels, thereby preventing the amplification of errors and maintaining the model's performance and generalization capabilities. How would you alter the code that generates the pseudo-labels to achieve this?\n",
    "\n",
    "## Iterative student-teacher training\n",
    "In the lecture, we discussed how iterative training (using the student to become the new teacher and repeating the pseudolabel process) progressively refines pseudo-labels and model robustness by cycling improvements between the student and teacher, enhancing overall performance over time. How would you alter the code above to incorporate iterative training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45e776-4845-47af-9be4-1b6c5cbe7380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
