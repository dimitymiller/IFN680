{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# Diving into Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "Let's load in any libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743d496-b334-499f-b57f-da86a441f925",
   "metadata": {},
   "source": [
    "# Part 0: Prepare the Data\n",
    "## Loading the Data\n",
    "The torchvision library already has a function for the FashionMNIST dataset that lets us load it in really easily!\n",
    "\n",
    "There's nothing for you to change in the cell below, but have a close look at what I'm doing as it'll be relevant for next week's practical and for Project 1.\n",
    "\n",
    "First, I'm creating a 'transform', this is a transformation that gets applied to the raw data. I'm doing 2 things: \n",
    "1. **converting the data to a Tensor**: this is the format that all data needs to be in before using the Pytorch library\n",
    "2. **normalizing the data**: the first value is the mean of the data and the second is the standard deviation. Note here that I've just assumed these values are 0.5 -- it's a kind of reasonable assumption, these are images with values between 0 and 1. In practice, I'd probably want to find the real mean and standard deviation for best performance.\n",
    "\n",
    "Next, I'm loading the FashionMNIST train and test subsets. Note the arguments I'm using: *root* is where the data will be, *train* is if I want to the train subset or instead test subset, *download* allows the library to download the data to the root if it's not already there, and *transform* will apply my defined transform to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14c9ab-3029-4195-932f-af7ee5666c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "trainval_dataset = torchvision.datasets.FashionMNIST(root = '../data/', train=True, download=True, transform = transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root = '../data/', train=False, download=True, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196f980-0c5d-4d13-b26e-e9df4d22ced7",
   "metadata": {},
   "source": [
    "## Creating Data Subsets\n",
    "\n",
    "Notice that we didn't have a validation dataset? FashionMNIST only has a training and testing subset, we'll need to create our own validation dataset by carving out some from the training subset. I've defined the number of data points I want in the training dataset and validation dataset with the train_size and val_size variables.\n",
    "\n",
    "**Your turn:** Investigate the torch.utils.data.random_split() function, and use it to randomly split the trainval_dataset into a train_dataset and val_dataset.\n",
    "\n",
    "If you've done this correctly, you should have a train dataset of size 48000, val size of 12000, and test size of 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abece46-c7b2-4468-916f-4a2ae29055b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_portion = 0.8\n",
    "train_size = int(train_portion*len(trainval_dataset))\n",
    "val_size = len(trainval_dataset)-train_size\n",
    "\n",
    "##### Your code goes here ######\n",
    "\n",
    "print(f'Size of train dataset: {len(train_dataset)}')\n",
    "print(f'Size of val dataset: {len(val_dataset)}')\n",
    "print(f'Size of test dataset: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0743b-a669-46ce-bedc-3c4e524d6562",
   "metadata": {},
   "source": [
    "## Inspecting the Data\n",
    "Below, I'm visualising the balance of different classes between the training and validation dataset. There's nothing for you to change here, just observe the histogram -- are the classes generally balanced between the training and validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967125d7-d2fe-40de-977f-a9722ca647d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = [data[1] for data in train_dataset] #this is a list comprehension -- one of the coolest things in python. Google it if you don't know what it is.\n",
    "val_labels = [data[1] for data in val_dataset]\n",
    "\n",
    "plt.hist([train_labels, val_labels], density = True) #normalize the histogram as we don't care about absolute numbers here, but a relative comparison\n",
    "plt.xlabel('Class')\n",
    "plt.xticks([i for i in range(10)]) #10 classes\n",
    "plt.ylabel('Normalized count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863df2d-9ddc-404e-9602-265d543e4cbf",
   "metadata": {},
   "source": [
    "Last step before we move onto creating a neural network -- let's visually inspect the data!\n",
    "\n",
    "There's one major take-away here: a neural network needs a vector of input values, but an image is a matrix of pixels (we'll learn more about this next week).\n",
    "For now, we're going to flatten that matrix into one long vector of pixel values -- weird right? But you'll be suprised at how well the neural network can learn from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e5616-2384-4a38-88c3-cc09c6bc5501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4)\n",
    "for idx in range(4):\n",
    "    train_image = (train_dataset[idx][0].numpy().reshape((28, 28))*255)+125\n",
    "    ax[0, idx].imshow(train_image, cmap = 'gray')\n",
    "    network_input = train_image.reshape(1, -1)\n",
    "    ax[1, idx].imshow(network_input, cmap = 'gray', aspect='auto')\n",
    "\n",
    "    ax[0, idx].set_axis_off()\n",
    "    ax[1, idx].set_axis_off()\n",
    "    \n",
    "ax[0, 1].set_title('Images from dataset')\n",
    "ax[1, 1].set_title('Input to the network -- a vector of pixel values')\n",
    "plt.show()\n",
    "\n",
    "print('Example input to the network without visualisation (first 100 values):')\n",
    "print(train_dataset[0][0].flatten()[:100])\n",
    "\n",
    "input_size = len(train_dataset[0][0].flatten())\n",
    "print(f'Size of input: {input_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601586a-f10f-4655-8f33-2ee7bf81595c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: A neural network with 1 hidden layer\n",
    "\n",
    "In this section, I'm going to show you how to build a neural network with 1 hidden layer. Most of the code is done for you, but make sure to read through and understand what's going on, because in the next part you'll be following this process to create a deep neural network!\n",
    "\n",
    "## Building the network\n",
    "In the cell below, there's a class called *SimpleNet*.     \n",
    "This is a neural network with 1 hidden layer, where the hidden layer has 128 neurons with a ReLU activation function. The output layer has 10 neurons and no activation function.\n",
    "\n",
    "The class definitions for a PyTorch model generally follow this minimum structure:\n",
    "1. an **\\__init__()** function where we create the model architecture.\n",
    "    1. **nn.Linear** creates a layer of linear neurons (no activation function - yet). This will be initialised with a weights vector of size nxm, where n is the size of the input to the layer, and m is the output size of the layer (i.e. number of neurons in the layer).  \n",
    "    Note the input arguments -- the first argument is the input size, the second argument is the number of neurons in the layer. I've initialised the first layer with an input_size of 784 -- above, we saw that was the size of an image when it was flattened to a vector. The second layer has an output size of 10 -- this is because we have 10 classes. I chose the first hidden layer to have 128 neurons (output size of 128) because it seemed like a reasonable interpolation between 784 and 10.  \n",
    "    Read more about this layer type here: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    2. **nn.ReLU()** creates a ReLU function - the non-linear activation function we will apply to all neurons after their linear operation.  \n",
    "    Read more about this layer type here: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "2. a **foward()** function where we pass an input through our model and return its output.\n",
    "    1. In the \\__init__ function, we created the layers that make our architecture. Now we must apply them sequentially.\n",
    "    2. Note how we use the ReLU in between the 2 linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1e7d4-0eda-46cd-bd01-a096e4e4defa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        y = self.fc2(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0bdad-56bc-4abf-a65e-50294424654b",
   "metadata": {},
   "source": [
    "Now that we've defined the network, let's create an instance of it. We're also *hopefully* using a GPU node at the moment, so we'll load the network onto the GPU if we are. If not, don't worry -- the network will stay on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33860f8-fd41-461e-88f4-0caf4beed284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "\n",
    "#create network and load onto device\n",
    "net = SimpleNet()\n",
    "net.to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a41060-db2e-4db8-a1bf-f6346a41f146",
   "metadata": {},
   "source": [
    "## Creating dataloaders\n",
    "\n",
    "Dataloaders are Pytorch's useful way of handling data. They automatically batch the dataset into the batch size you want, and can also randomly shuffle the data for you if you choose.\n",
    "\n",
    "1. *torch.utils.data.DataLoader()* You can read the documentation here: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "    1. First argument is the dataset.\n",
    "    2. Optional argument *batch_size* is the batch size to test the model with.\n",
    "    3. Optional argument *shuffle* controls whether data is randomly shuffled before taking from the dataset.\n",
    "    4. Optional argument *num_workers* is how many subprocesses are used to load data from the dataset -- it can make loading the data faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6eb38-0f61-4c0a-b436-1a2eceada5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers = 2)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers = 2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195db6c-bc65-401b-8cb7-19385dab02a8",
   "metadata": {},
   "source": [
    "## Creating a loss function and optimizer\n",
    "\n",
    "We're going to use Cross Entropy Loss (a very standard loss for classification tasks) and perform mini-batch Stochastic Gradient Descent.\n",
    "\n",
    "Nothing too exciting here:\n",
    "1. *nn.CrossEntropyLoss* - plain Cross Entropy Loss. You can read the documentation here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "2. *optim.SGD* - stochastic gradient descent! Documentation here: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "    1. It will work with however many examples you pass in (anywhere from 1 - entire dataset). \n",
    "    2. *lr* is the learning rate. Generally somewhere between 0.01-0.001 is a sensible first guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c508fa-274f-403f-a2a8-4bce588f8f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9225f-3e9e-492e-9635-7c3bd596594f",
   "metadata": {},
   "source": [
    "## Time to train!\n",
    "\n",
    "In the cell below, we're training the neural network (NN) for 10 epochs. This should take about 2 minutes if you're on the GPU, and about 4 minutes if you're on the CPU.\n",
    "\n",
    "Notice the general flow of the training process:\n",
    "1. For every epoch:\n",
    "    1. We iterate through each batch in our trainloader\n",
    "        1. We separate the inputs from the GT class labels\n",
    "        2. Move inputs and GT labels to the GPU if available\n",
    "        3. Do any remaining preprocessing to the data (in this case, flatten an image into a vector).\n",
    "        4. Zero the parameter gradients -- this is to ensure we start 'fresh' for each step of gradient descent.\n",
    "        5. Forward pass through the network to find the prediction\n",
    "        6. Calculate the loss between the prediction and GT labels\n",
    "        7. Calculate the gradients with a backwards pass\n",
    "        8. Change the parameters based on the gradients, using the optimizer\n",
    "        9. Record any data we want to save\n",
    "        \n",
    "This is the general flow for the training process, with some important elements missing... before we get to that, run the training process, and observe the loss curve produced at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57f584-c9b2-4eb0-866e-584c364ddd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = {'train': []}\n",
    "total_epochs = 10\n",
    "\n",
    "for epoch in tqdm.tqdm(range(total_epochs), desc=\"Training progress\"):    \n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for i, data in  enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #flatten the images into a vector\n",
    "        inputs = inputs.reshape(len(inputs), -1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass to find the outputs\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        #calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #backward pass to calculate the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #take a step with gradient descent to change the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #let's keep track of the loss\n",
    "        train_loss += [loss.cpu().item()]\n",
    "\n",
    "    #record the mean loss over the entire epoch\n",
    "    epoch_loss = np.mean(train_loss)\n",
    "    losses['train'] += [epoch_loss]\n",
    "    \n",
    "#let's plot the loss for each epoch to observe how it changes over 10 epochs\n",
    "plt.plot(losses['train'], label = 'Train')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179325c7-0894-454d-8644-102ad310e715",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "There are a number of key elements missing from the cell above. I've pasted the same code below. One-by-one, add each of these elements to the below cell. At the end, you should produce 2 loss plots that look similar to the below.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"figures/simplenet_curves.png\" />\n",
    "</p>\n",
    "\n",
    "1. There is currently no regularisation being applied to the weights. The optimizer has a useful argument that does this for us -- look at the documentation for the SGD optimizer and the weight_decay argument. Use a value between 0.001-0.0001.\n",
    "2. The loss itself is not very interpretable. Create another plot that shows the average accuracy on the training data at each epoch.\n",
    "    1. You will need to find the predicted class by using the maximum class score in outputs for each input. The torch.argmax() function will be useful.\n",
    "    2. You will need to compare the predicted class to the GT labels stored in the labels variable.\n",
    "3. Comparison to the validation dataset in each epoch.\n",
    "    1. After the loop that tests the trainloader, add a loop that tests the valloader.\n",
    "    **It is critical that you do not perform a backward pass of optimizer step on the validation data. We do not want to learn from the validation data, only measure performance.**\n",
    "    2. Store the loss and accuracy for the validation data, and plot it on the same plots as the training datasets.\n",
    "\n",
    "After you've done the above, consider: At what point has the model converged?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9210657-290b-4882-8f0d-d452e80ae55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Re-create network and load onto device\n",
    "net = SimpleNet()\n",
    "net.to(device)\n",
    "\n",
    "#because we have re-created the network, we need to re-initialise the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "losses = {'train': []}\n",
    "\n",
    "total_epochs = 10\n",
    "\n",
    "for epoch in tqdm.tqdm(range(total_epochs), desc=\"Training progress\"):    \n",
    "    \n",
    "    train_loss = []\n",
    "    for i, data in  enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #flatten the images into a vector\n",
    "        inputs = inputs.reshape(len(inputs), -1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass to find the outputs\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        #calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #backward pass to calculate the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #take a step with gradient descent to change the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #let's keep track of the loss\n",
    "        train_loss += [loss.cpu().item()]\n",
    "\n",
    "    #record the mean loss over the entire epoch\n",
    "    epoch_loss = np.mean(train_loss)\n",
    "    losses['train'] += [epoch_loss]\n",
    "    \n",
    "    \n",
    "#let's plot the loss for each epoch to observe how it changes over 5 epochs\n",
    "plt.plot(losses['train'], label = 'Train')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d4f1c-c652-4031-aea6-cc2763daeebe",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "## Build a better model\n",
    "Building off the code from above, create a deep neural network with at least 4 fully connected layers.\n",
    "\n",
    "As a starting point, you may choose to use:\n",
    "\n",
    "However, the choice is yours if you want to try something different!\n",
    "\n",
    "Train your model and explore these different variables, observing how they influence the attainable validation accuracy. \n",
    "1. Experiment with changing the learning rate. Observe how it changes performance.\n",
    "    1. Experiment with changing the learning rate while learning, e.g. after 10 epochs, re-initialise the optimizer with a lower learning rate.\n",
    "2. Try changing the network depth and layer widths and observe any changes in performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687f69f-59ff-4f7c-b0ba-2da2eb178a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Update this model #####\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        y = self.fc2(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd842a1-991e-483e-a460-aa6c8cfc193b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Enter your training code from earlier here #####\n",
    "#### Make sure to use the new DeepNet class #####\n",
    "\n",
    "#Re-create network and load onto device\n",
    "net = DeepNet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73e2fa-963e-4086-9100-ab7e9d09ab2c",
   "metadata": {},
   "source": [
    "## Evaluate your best model on the test dataset\n",
    "Once you're satisfied that you've found the optimal hyperparameters, train your model with those hyperparameters (making sure to stop before overfitting) and test for accuracy on the test dataset.\n",
    "\n",
    "The best model I could get was 89.75%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428bb9d-f50e-4cc5-a86d-c21d0f2e1227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Enter your code for testing accuracy with the testloader here.\n",
    "\n",
    "print(f'Accuracy on test dataset: {}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a61f4-947f-4455-adea-258c2867096e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
